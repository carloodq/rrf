{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxYRcyas1A60"
      },
      "source": [
        "# Preprocessing functions and corpus preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k76yQWiX1jyC"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain --quiet\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m spacy download it_core_news_sm\n",
        "!pip install rank-bm25\n",
        "!pip install  python-dotenv\n",
        "!pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vQYxhGV_1VoJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "file_path='docs/test.json'\n",
        "data = json.loads(Path(file_path).read_text())\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "tbr_text =list(df.data)\n",
        "text = tbr_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJT0ChgR1Ibo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_stopwords(text):\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    example_sent = text\n",
        "    en_sw = set(stopwords.words('english'))\n",
        "    stop_words = en_sw.union(it_sw)\n",
        "    word_tokens = word_tokenize(example_sent)\n",
        "    # converts the words in word_tokens to lower case and then checks whether\n",
        "    #they are present in stop_words or not\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    #with no lower case conversion\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return  ' '.join(filtered_sentence)\n",
        "\n",
        "\n",
        "def lemmatize(text):\n",
        "    import spacy\n",
        "    import it_core_news_sm\n",
        "    nlp = it_core_news_sm.load()\n",
        "    # Define a sample text\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    # Extract lemmatized tokens\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    # Join the lemmatized tokens into a sentence\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "def preprocess(text):\n",
        "    return lemmatize(remove_stopwords(text.lower()))\n",
        "\n",
        "# preprocess_corpus\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# text = text.values()\n",
        "preprocessed = [preprocess(i) for i in text]\n",
        "import pandas as pd\n",
        "pd.DataFrame(preprocessed).to_csv(\"tbr.csv\")\n",
        "list(pd.read_csv(\"tbr.csv\")[\"0\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrKZQrCv9ni"
      },
      "source": [
        "# Final pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8WAkVsCv_Xh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_idx_rank_query(path_to_corpus, query):\n",
        "  # 1.1 Tokenizing\n",
        "  from rank_bm25 import BM25Okapi\n",
        "  tbrl = list(pd.read_csv(path_to_corpus)[\"0\"].values)\n",
        "  corpus = list(tbrl)\n",
        "  tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "  # 1.2 preprocessing query\n",
        "  query = preprocess(query)\n",
        "  # 2. querying\n",
        "  tokenized_query = query.split(\" \")\n",
        "  doc_scores_bm25 = bm25.get_scores(tokenized_query)\n",
        "  idx_rank_query = pd.DataFrame(doc_scores_bm25, columns =[\"ds\"]).sort_values(by = \"ds\", ascending = False).reset_index(names = \"dn\").dn.values\n",
        "  return idx_rank_query\n",
        "\n",
        "def get_idx_rank_semantic(path_to_corpus, query):\n",
        "  from google.colab import userdata\n",
        "  import os\n",
        "  os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "  # from dotenv import load_dotenv\n",
        "  from langchain.text_splitter import CharacterTextSplitter\n",
        "  from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "  from langchain.vectorstores import FAISS\n",
        "  from langchain.chains.question_answering import load_qa_chain\n",
        "  from langchain.llms import OpenAI\n",
        "  from langchain.callbacks import get_openai_callback\n",
        "  from langchain.document_loaders import CSVLoader\n",
        "  loader = CSVLoader(file_path=path_to_corpus)\n",
        "  data = loader.load()\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  docsearch = FAISS.from_documents(data, embeddings)\n",
        "  retriever = docsearch.as_retriever(search_kwargs={\"k\": len(data)})\n",
        "  docs = retriever.get_relevant_documents(query)\n",
        "  rank_semantic = [docs[i].metadata['row'] for i in range(len(docs))]\n",
        "  idx_rank_semantic = pd.DataFrame(rank_semantic, columns =[\"rank_semantic\"]).sort_values(by = \"rank_semantic\").index\n",
        "  return idx_rank_semantic\n",
        "\n",
        "def get_rrf_top_k(path_to_corpus, query, k = 3 ):\n",
        "  idx_rank_semantic = get_idx_rank_semantic(path_to_corpus, query)\n",
        "  idx_rank_query =  get_idx_rank_query(path_to_corpus, query)\n",
        "  idx_rank_rrf = []\n",
        "  for i in range(len(idx_rank_query)):\n",
        "      idx_rank_rrf.append(1/(idx_rank_semantic[i]+1) + 1/(idx_rank_query[i]+1))\n",
        "  rrf_top_k = pd.DataFrame(idx_rank_rrf, columns =[\"rank_rrf\"]).sort_values(by = \"rank_rrf\", ascending = False).sort_values('rank_rrf', ascending = False).head(k).index\n",
        "  return rrf_top_k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D65mzAU3ygEW"
      },
      "outputs": [],
      "source": [
        "query = \"\"\n",
        "path_to_corpus = \".csv\"\n",
        "rrf_top_k = get_rrf_top_k(path_to_corpus, query, k = 3 )\n",
        "tbrl = list(pd.read_csv(path_to_corpus)[\"0\"].values)\n",
        "print(tbr_text[rrf_top_k[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIKgNxci_fgD"
      },
      "outputs": [],
      "source": [
        "  # Come usarlo? Fai una query relativa ai TBR nella seconda cella\n",
        "  query = \"\"\n",
        "  path_to_corpus = \".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZOpKQDu-f0A"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, path_to_corpus):\n",
        "\n",
        "\n",
        "  rrf_top_k = get_rrf_top_k(path_to_corpus, query, k = 3 )\n",
        "  tbrl = list(pd.read_csv(path_to_corpus)[\"0\"].values)\n",
        "  context = tbr_text[rrf_top_k[0]]\n",
        "\n",
        "  # LangChain\n",
        "  from langchain.chat_models import ChatOpenAI\n",
        "  from langchain.schema import (\n",
        "      HumanMessage\n",
        "  )\n",
        "  model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "  # Initialize the chat object.\n",
        "  chat = ChatOpenAI(model_name=model_name, temperature=0)\n",
        "  question = f'''{query}\n",
        "  Context: {context}'''\n",
        "  print(context)\n",
        "\n",
        "\n",
        "  return chat([HumanMessage(content=question)]), context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h30aUV8a_Eu1"
      },
      "outputs": [],
      "source": [
        "answ = generate_answer(query, path_to_corpus)\n",
        "print(answ[0].content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psLdmxNeBmBc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "customchat",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
